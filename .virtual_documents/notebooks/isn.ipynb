import os 
import pandas as pd
import torch

device = "cuda:0" if torch.cuda.is_available() else "cpu"
os.chdir("..")
cwd = os.getcwd()
sep = os.sep 


cwd


expr_filename = cwd + sep + "use_case"+sep+"data"+sep+"bladder_cancer"+sep+"gene_expression_data_preprocessed.csv"
output = cwd + sep + "ouput.png"
expr = pd.read_csv(expr_filename, index_col = 0)
expr


targets = pd.DataFrame(expr["Target"])
expr = expr.drop("Target", axis = 1)


from adin import utils 
isns, interaction_df = utils.create_sparse_isns(expr, th = 0.998)
print(f"Interaction dataframe: {interaction_df}")
isns = torch.stack(isns).to(device)
print(isns.shape)


edge_list = [row["feature_1"]+"_"+row["feature_2"] for index, row in interaction_df.iterrows()]
# Parse the edges and create node mapping
source_nodes, target_nodes, node_mapping = utils.parse_edges(edge_list)
# Create edge_index tensor
edge_index = utils.create_edge_index(source_nodes, target_nodes)
    
y = torch.tensor(targets.values).to(device).flatten()
edge_index = edge_index.to(device)
print(y.shape, edge_index.shape)


x = []
n_graphs = isns.shape[1]
patients = list(expr.index)
nodes = list(node_mapping.keys())
for i in range(n_graphs):
    row = expr.values[i, :]
    idx_nodes = [i for i, node in enumerate(nodes) if node in expr.columns]
    masked_row = row[idx_nodes]
    x.append(torch.tensor(masked_row))
x = torch.stack(x)
x.shape


from adin.dl import create_torch_geo_data

def train_test_split_and_mask(data, node_mapping_rev, train_size = 0.2, isn = False):
    from sklearn.model_selection import train_test_split
    """
    Input:
        data: torch_geometric.Data object
        train_size: float between 0.0 and 1.0, OPTIONAL, default 0.2
    """
    #input 
    
    # Assuming `data.y` contains the labels
    num_nodes = data.num_nodes
    if train_size < 0.1 or train_size >= 1:
       raise Exception("Train size {} must be greater (or equal) then 0.1 and lower then 1".format(train_size))

    # Create a boolean mask for training nodes
    indices = torch.arange(num_nodes).to(device)
    train_indices, test_indices = train_test_split(indices, train_size=train_size, stratify=data.y.cpu())
    y = data.y
    
    #uqs, counts = torch.unique(y, return_counts = True)
    #start = int((2/3)*counts[0])
    #train_indices = torch.arange(0, start).to(device)
    #test_indices = torch.arange(start, num_nodes).to(device)

    train_mask = torch.zeros(num_nodes, dtype=torch.bool).to(device)
    test_mask = torch.zeros(num_nodes, dtype=torch.bool).to(device)
    train_mask[train_indices] = True
    test_mask[test_indices] = True

    data.edge_index = data.edge_index.to(device)

    data.train_mask = train_mask
    data.test_mask = test_mask

    if not isn:
        train_edgeindex = filter_edge_index(data.edge_index, data.train_mask)
        test_edgeindex = filter_edge_index(data.edge_index, data.test_mask)
    else:
        train_edgeindex = data.edge_index
        test_edgeindex = data.edge_index
        
    train_dataloader = create_torch_geo_data(data.x[data.train_mask], data.y[data.train_mask], train_edgeindex)
    test_dataloader = create_torch_geo_data(data.x[data.test_mask], data.y[data.test_mask], test_edgeindex)
    return train_dataloader, test_dataloader


# Filter the edge_index and ensure all edges reference valid nodes in the mask
def filter_edge_index(edge_index, mask):
    
    # Get the indices of valid nodes according to the mask
    idx_map = torch.nonzero(mask, as_tuple=True)[0].to(device)
    
    # Create a mapping from old indices to new indices
    node_idx_map = {old_idx.item(): new_idx for new_idx, old_idx in enumerate(idx_map)}

    # Filter edge_index to include only valid edges
    mask_edge_index = (mask[edge_index[0]] & mask[edge_index[1]]).nonzero(as_tuple=False).to(device).squeeze()
    
    if mask_edge_index.numel() == 0:
        return torch.empty((2, 0), dtype=torch.long).to(device)  # No valid edges

    # Filter and remap edge_index
    filtered_edge_index = edge_index[:, mask_edge_index]
    filtered_edge_index = torch.tensor([[node_idx_map.get(int(i), -1) for i in edge] for edge in filtered_edge_index.t()], dtype=torch.long).to(device).t()

    # Remove edges with -1 index due to remapping issues
    valid_edges = filtered_edge_index.min(dim=0).values >= 0
    filtered_edge_index = filtered_edge_index[:, valid_edges]

    return filtered_edge_index


from adin import gaan_config, dl

uqs, counts = torch.unique(y, return_counts = True)
dict_counts = {}
for uq, count in zip(uqs, counts):
    dict_counts[uq.item()] = count.item()
contamination = (dict_counts[1]/(dict_counts[0] + dict_counts[1]))*0.5
gpu = 0 if torch.cuda.is_available() else -1
gaan_params = gaan_config.GAAN_config(noise_dim=64, hid_dim=128, num_layers=2, dropout=0.3, contamination=contamination, lr = 0.00005, epoch = 200, gpu = -1, batch_size=1, verbose = 1, isn = True, th = 0.93)

x = x.cpu()
y = y.cpu()
edge_index = edge_index.cpu()
mydataloader = dl.create_torch_geo_data(x, y, edge_index)
node_mapping_rev = {value: key for key, value in node_mapping.items()}
mydataloader


print("Train - Test split")
device = "cpu"
dataloader_train, dataloader_test = train_test_split_and_mask(mydataloader, node_mapping_rev, train_size = 0.7, isn = True)
in_dim = dataloader_train.x.shape[1]
dataloader_train.x.shape


print("Create GAAN model")
model = dl.create_model(in_dim, gaan_params, isn = True)
model = model.to("cpu")
model = dl.train_gaan(model, dataloader_train)
models["GAAN_isn"] = model
if "Temp" in models.keys():
    del models["Temp"]
df_result = dl.create_results_df({"GAAN_isn": model}, dataloader_test)
df_result


preds = model.predict(dataloader_test).cpu()
explainer = None
explainers["GAAN_isn"] = explainer   
y_test = dataloader_test.y.cpu()
classes = {0: "Normal", 1: "Anomalous"}
cm = confusion_matrix(y_test, preds)
fig_cm = dl.plot_cm(classes, cm, "GAAN_isn")
fig_roc_test = ml.plot_roc_curve_(y_test, preds)
fig_cm.show()
fig_roc_test.show()






